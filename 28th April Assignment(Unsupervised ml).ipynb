{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
    "\n",
    "Hierarchical clustering is an unsupervised machine learning algorithm, which is used to group the unlabeled datasets into a cluster and also known as hierarchical cluster analysis or HCA. In this algorithm, we develop the hierarchy of clusters in the form of a tree, and this tree-shaped structure is known as the dendrogram. It is a method works via grouping data into a tree of clusters. In Hierarchical Clustering, the aim is to produce a hierarchical series of nested clusters. A diagram called **Dendrogram** (A Dendrogram is a tree-like diagram that statistics the sequences of merges or splits) graphically represents this hierarchy and is an inverted tree that describes the order in which factors are merged (bottom-up view) or clusters are broken up (top-down view).\n",
    "\n",
    "Hierarchical clustering is different from other clustering techniques such as k-means or DBSCAN because it does not require specifying the number of clusters beforehand, it can handle non-convex clusters and clusters of different sizes and densities, and it can reveal the hierarchical structure of the data. However, it also has some drawbacks such as high computational cost, memory requirements, and sensitivity to the choice of distance metric and linkage criterion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
    "\n",
    "Two main types of hierarchical clustering algorithms are:\n",
    "1. Agglomerative clustering: Initially consider every data point as an individual Cluster and at every step, merge the nearest pairs of the cluster. (It is a bottom-up method). At first, every dataset is considered an individual entity or cluster. At every iteration, the clusters merge with different clusters until one cluster is formed. \n",
    "\n",
    "The algorithm for Agglomerative Hierarchical Clustering is:\n",
    "\n",
    "Step 1: Calculate the similarity of one cluster with all the other clusters (calculate proximity matrix)\n",
    "Step 2: Consider every data point as an individual cluster\n",
    "Step 3: Merge the clusters which are highly similar or close to each other.\n",
    "Step 4: Recalculate the proximity matrix for each cluster\n",
    "Step 5: Repeat Steps 3 and 4 until only a single cluster remains.\n",
    "\n",
    "2. Divisive clustering: It is a top-down approach. We can say that Divisive Hierarchical clustering is precisely the opposite of Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we take into account all of the data points as a single cluster and in every iteration, we separate the data points from the clusters which arenâ€™t comparable. In the end, we are left with N clusters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n",
    "\n",
    "The closest distance between the two clusters is crucial for the hierarchical clustering. There are various ways to calculate the distance between two clusters, and these ways decide the rule for clustering. These measures are called Linkage methods. Some of the popular linkage methods are:\n",
    "\n",
    "1. Single Linkage: It is the Shortest Distance between the closest points of the clusters.\n",
    "2. Complete Linkage: It is the farthest distance between the two points of two different clusters. It is one of the popular linkage methods as it forms tighter clusters than single-linkage.\n",
    "3. Average Linkage: It is the linkage method in which the distance between each pair of datasets is added up and then divided by the total number of datasets to calculate the average distance between two clusters. It is also one of the most popular linkage methods.\n",
    "4. Centroid Linkage: It is the linkage method in which the distance between the centroid of the clusters is calculated.\n",
    "\n",
    "Some of the common distance metrics used:\n",
    "\n",
    "- Euclidean distance: The square root of the sum of squared differences between corresponding coordinates of two points.\n",
    "- Manhattan distance: The sum of absolute differences between corresponding coordinates of two points.\n",
    "- Cosine similarity: The cosine of the angle between two vectors representing two points.\n",
    "- Correlation: The normalized measure of how well two variables are linearly related."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n",
    "\n",
    "some common methods used to determine the optimal number of clusters are:\n",
    "\n",
    "- Dendrogram Visualization: Hierarchical clustering produces a dendrogram that illustrates the merging and splitting of clusters at different similarity levels. By visually inspecting the dendrogram, you can identify distinct clusters based on the vertical distances between merging points. The number of clusters can be determined by finding an appropriate level where merging occurs. We can determine the number of cluster by selecting the longest vertical line such that no horizontal line passes through it, in this vertical line we draw a horizontal line and check the number of vertical line it crosses, the value of k is equal to the number of vertical lines through which horizontal line passes.\n",
    "\n",
    "- Distance-Based Methods: These methods involve analyzing the dissimilarity or distance measures between data points. Two commonly used approaches are:\n",
    "    - Elbow Method: Plot the within-cluster sum of squares (WCSS) or the average linkage distance against the number of clusters. Look for an \"elbow\" point on the plot where the improvement in clustering quality diminishes significantly, indicating the optimal number of clusters.\n",
    "    - Silhouette Analysis: Compute the silhouette coefficient for each data point, which measures how well it belongs to its own cluster compared to neighboring clusters. Calculate the average silhouette score across different numbers of clusters and choose the number that maximizes the score.\n",
    "\n",
    "- The gap statistic: This method compares the total WCSS for different values of k with their expected values under an appropriate reference null distribution of the data. The idea is to find a balance between the WCSS and the complexity of the model. The optimal number of clusters is the one that maximizes the gap statistic, which is the difference between the observed and the expected WCSS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "\n",
    "The dendrogram is a tree-like structure that is mainly used to store each step as a memory that the Hierarchical clustering algorithm performs. It is a type of tree diagram that shows the hierarchical relationship between objects or groups of objects based on their distance or similarity. It is often used to visualize the results of hierarchical clustering, which is a method of unsupervised learning that groups similar data points into clusters based on their distance or similarity.\n",
    "\n",
    "A dendrogram is useful for analyzing the results of hierarchical clustering because it can:\n",
    "\n",
    "- Show how the clusters are formed by merging or splitting smaller clusters at different levels of hierarchy.\n",
    "- Indicate the distance or similarity between clusters or objects by the height or length of the branches that connect them3.\n",
    "- Help to determine the optimal number of clusters by looking for a point where the branches form a large gap or an \"elbow\".\n",
    "- Provide a visual summary of the data and reveal patterns or outliers that may not be obvious from other methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n",
    "\n",
    "Yes, hierarchical clustering can be used for both numerical and categorical data. \n",
    "\n",
    "For numerical data:</br>\n",
    "Distance metrics commonly used for numerical data include:\n",
    "\n",
    "- Euclidean Distance: This metric calculates the straight-line distance between two data points in a multi-dimensional space. It assumes that the data features are continuous and follow a Euclidean geometry.\n",
    "- Manhattan Distance: Also known as city block or L1 distance, this metric measures the sum of absolute differences between the coordinates of two data points. It is suitable for data with features that represent distances or counts.\n",
    "- Cosine Distance: This metric measures the angle between two data points represented as vectors. It is often used for high-dimensional numerical data where the magnitude of the vector is less important than the direction.\n",
    "\n",
    "For categorical data:</br>\n",
    "Some commonly used metrics for categorical data include:\n",
    "\n",
    "- Hamming Distance: This metric counts the number of positions at which two categorical data points differ. It is suitable for binary or multi-class categorical variables.\n",
    "- Jaccard Distance: This metric measures the dissimilarity between two sets of binary variables. It is useful for categorical data represented as binary vectors, such as presence or absence of certain features.\n",
    "- Gower's Distance: This metric can handle a mix of categorical and numerical variables. It calculates the dissimilarity as a weighted combination of different distance measures, depending on the data types."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "\n",
    "Use hierarchical clustering to identify outliers in data:\n",
    "\n",
    "- One way to use hierarchical clustering to identify outliers or anomalies in data is to use a clustered hierarchical anomaly and outlier detection algorithm (CHAODA). This algorithm uses a fast hierarchical clustering technique to create a graph from the cluster tree, based on overlapping clusters and their geometric and topological features. Then, it explores various properties of the graph and its clusters to find outliers. \n",
    "\n",
    "- Another way to use hierarchical clustering to identify outliers or anomalies in data is to use an agglomerative hierarchical clustering algorithm. This algorithm starts with each data point as a single cluster and then merges the closest clusters until a desired number of clusters is reached. The algorithm also supplies a set of outliers that are not merged with any cluster and are considered as anomalies. The outliers can be detected based on some criteria such as distance, density, or connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
