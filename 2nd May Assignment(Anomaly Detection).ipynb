{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "Anomaly detection is a process of finding those rare items, data points, events, or observations that make suspicions by being different from the rest data points or observations. Anomaly detection is also known as outlier detection. It  is a step in data mining that identifies data points, events, and/or observations that deviate from a datasetâ€™s normal behavior. Anomalous data can indicate critical incidents, such as a technical glitch, or potential opportunities, for instance, a change in consumer behavior. Machine learning is progressively being used to automate anomaly detection.\n",
    "\n",
    "The purpose of anomaly detection is to discover and analyze the causes and consequences of the anomalies, and to take appropriate actions to prevent or mitigate them. Anomaly detection can also be used to improve the quality and accuracy of the data and the models that use them.\n",
    "\n",
    "Purpose and application of anomaly dectection:\n",
    "- Fraud detection: Anomaly detection plays a crucial role in identifying fraudulent transactions, such as credit card fraud, insurance fraud, or identity theft. It helps distinguish fraudulent activities from legitimate ones by detecting unusual patterns or behaviors.\n",
    "- Performance monitoring: Anomaly detection is employed to monitor performance metrics in various fields, including finance, healthcare, and transportation. It helps identify performance deviations, such as sudden drops or spikes in stock prices, abnormal patient vitals, or traffic congestion patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "Some of the key challenges in anomaly detection are:\n",
    "\n",
    "- Data quality: The quality of the underlying data can affect the accuracy and reliability of the anomaly detection model. Data quality problems can include null values, inconsistent formats, duplicate data, different scales of measurement, and human errors. \n",
    "\n",
    "- Data sparsity: The scarcity of labeled data or anomalous data can make it difficult to train and validate the anomaly detection model. Anomalies are rare by definition, so there may not be enough examples to learn from or to evaluate the performance of the model. \n",
    "\n",
    "- Data complexity: The complexity of the data can pose challenges for anomaly detection, especially for high-dimensional or time-series data. High dimensionality can result in data sparsity, curse of dimensionality, and computational inefficiency. Time-series data can have seasonality, trends, noise, and contextual anomalies that require sophisticated methods to handle. \n",
    "\n",
    "- Model selection: The choice of the anomaly detection model can depend on various factors such as the type of data, the type of anomalies, the availability of labels, the assumptions and constraints of the model, and the evaluation metrics. There is no one-size-fits-all solution for anomaly detection, so finding the best model for a given problem can be challenging. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Unsupervised anomaly detection differs from supervised anomaly detection in the following ways:\n",
    "\n",
    "- Unsupervised anomaly detection does not require any labeled data that indicates if a record is normal or abnormal. It assumes that anomalies are rare and make up a small percentage of the data. It involves modeling the normal data distribution and defining a measurement to classify samples as anomalous or normal. Supervised anomaly detection requires labeled data that represents previous failures or anomalies. It involves training a binary classifier that can distinguish between normal and abnormal samples based on the labels.\n",
    "\n",
    "- Unsupervised anomaly detection is more applicable to most real-world problems, where labeled data is scarce or expensive to obtain. Supervised anomaly detection is more suitable for problems where labeled data is available and reliable.\n",
    "\n",
    "- Unsupervised anomaly detection is more challenging and complex than supervised anomaly detection, as it requires finding a suitable model, parameters, and threshold for detecting anomalies without any feedback. Supervised anomaly detection is more straightforward and easier to evaluate, as it can use standard classification techniques and metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "Some main categories of anomaly detection algorithm are:\n",
    "- Distance-based anomaly detection: These algorithms assume that the normal data points are close to each other and use measures of distance to detect outliers that are far away from their nearest neighbors. Examples of such algorithms are K-Means, K-Medoids, and Hierarchical Clustering.\n",
    "\n",
    "- Clustering-based anomaly detection: This algorithm assume that the normal data points belong to one or more clusters and use clustering techniques to detect outliers that do not belong to any cluster. These methods assign data points to clusters based on detected similarities. K-means is a popular example, where outliers are determined by how far they extend from a cluster group.\n",
    "\n",
    "- Bayesian-network anomaly detection: This method work by defining the probability that an event will occur based on the presence of contributing factors and detecting relationships with the same root cause.\n",
    "\n",
    "- Isolation-based anomaly detection: This algorithm assume that the normal data points are easy to isolate from the rest of the data and use random partitioning techniques to detect outliers that are hard to isolate. Examples of such algorithms are Isolation Forest, Random Cut Forest, and Extended Isolation Forest.\n",
    "\n",
    "- Statistical anomaly detection: This algorithm assume that the normal data follows a certain statistical distribution and use statistical tests to detect outliers that deviate from that distribution. Examples of such algorithms are Z-score, Grubbs' test, and Chi-square test.\n",
    "\n",
    "- Density-based anomaly detection: These algorithms assume that the normal data points are densely clustered in a high-dimensional space and use measures of density to detect outliers that lie in low-density regions. Examples of such algorithms are Local Outlier Factor (LOF), K-Nearest Neighbors (KNN), and Density-Based Spatial Clustering of Applications with Noise (DBSCAN). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Main assumptions made by distance-based anomaly detection method are:\n",
    "\n",
    "- The data is not noisy or sparse, as noise and sparsity can affect the distance calculations and lead to false positives or false negatives.\n",
    "- The data is not high-dimensional, as high-dimensional data can suffer from the curse of dimensionality and make the distance measures less meaningful.\n",
    "- The data is not multimodal, as multimodal data can have multiple clusters of normal points and make it hard to define a global distance threshold for outliers.\n",
    "- The data is not skewed, as skewed data can have outliers that are close to the majority of the data points and make them hard to detect.\n",
    "- The data is not correlated, as correlated data can have outliers that are close to the normal points along some dimensions but far away along others and make them hard to detect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "Local outlier factor (LOF) is an algorithm used for Unsupervised outlier detection. It produces an anomaly score that represents data points which are outliers in the data set. It does this by measuring the local density deviation of a given data point with respect to the data points near it. \n",
    "\n",
    "The formula for LOF anomaly score is given by:\n",
    "\n",
    "$LOF(k) = \\frac{\\sum_{o \\in N_k(p)} \\frac{LRD_k(o)}{LRD_k(p)}}{|N_k(p)|}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $k$ is the number of nearest neighbors\n",
    "- $p$ is the data point of interest\n",
    "- $N_k(p)$ is the set of $k$ nearest neighbors of $p$\n",
    "- $LRD_k(p)$ is the local reachability density of $p$, defined as the inverse of the average reachability distance of $p$ from its neighbors\n",
    "- $LRD_k(o)$ is the local reachability density of a neighbor $o$ of $p$\n",
    "- $|N_k(p)|$ is the cardinality of the set of neighbors of $p$\n",
    "\n",
    "The LOF score measures how isolated a data point is from its neighbors. A high LOF score indicates an outlier, while a low LOF score indicates a normal point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "- n_estimators: The number of base estimators in the ensemble. A larger number of trees improves the accuracy of the anomaly score, but also increases the computational cost and memory usage.\n",
    "\n",
    "- max_samples: The number of samples to draw from the data to train each base estimator. A smaller number of samples improves the isolation of anomalies, but also reduces the diversity of the trees and may cause overfitting.\n",
    "\n",
    "- contamination: The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.  A higher contamination value means a lower threshold and more outliers. contamination should be in the range (0, 0.5].\n",
    "\n",
    "- max_features: The number of features to draw from the data to train each base estimator. A smaller number of features improves the isolation of anomalies, but also reduces the information available for splitting and may cause underfitting.\n",
    "\n",
    "- bootstrap: A boolean flag that indicates whether to use bootstrap sampling with replacement or not.  Bootstrap sampling can improve the diversity of the trees and reduce the variance, but also increases the bias and may miss some outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "\n",
    "The KNN algorithm computes anomaly scores by measuring the distance to the k-th nearest neighbor of a given data point. The formula for KNN anomaly score is given by:\n",
    "\n",
    "$$AS(p) = d(p, N_k(p))$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $p$ is the data point of interest\n",
    "- $N_k(p)$ is the k-th nearest neighbor of $p$\n",
    "- $d$ is the distance function (e.g. Euclidean, Manhattan, etc.)\n",
    "\n",
    "The anomaly score of a data point is defined as the average distance to its k nearest neighbors. In this case, k = 10, so we need to find the 10 closest points to the data point and calculate their average distance. The data point has only 2 neighbors of the same class within a radius of 0.5, it means that the other 8 neighbors are of a different class and are likely to be farther away. Therefore, the anomaly score will be high, indicating that the data point is an outlier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
    "\n",
    "The Isolation Forest algorithm computes anomaly scores by measuring the average path length of a given data point from the root node to the terminating node of each tree in the forest. The formula for Isolation Forest anomaly score is given by:\n",
    "\n",
    "$s(x, n) = 2^{-\\frac{E(h(x))}{c(n)}}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x$ is the data point of interest\n",
    "- $n$ is the number of samples used for training\n",
    "- $h(x)$ is the average path length of $x$ over all trees\n",
    "- $c(n)$ is the average path length of unsuccessful searches in a binary search tree\n",
    "\n",
    "\n",
    "We have 100 trees and 3000 data points, then the average path length of unsuccessful searches in a binary search tree is given by:\n",
    "\n",
    "$c(3000) \\approx 2H(3000 - 1) - 2(3000 - 1)/3000 \\approx 10.62$\n",
    "\n",
    "where $H$ is the harmonic number.\n",
    "\n",
    "Data point has an average path length of 5.0 over all trees, then its anomaly score is given by:\n",
    "\n",
    "$s(x, 3000) = 2^{-\\frac{5.0}{10.62}} \\approx 0.42$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
